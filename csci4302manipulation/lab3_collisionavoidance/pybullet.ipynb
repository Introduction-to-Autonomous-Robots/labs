{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a737c-9895-4252-ae89-35a09782c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install PyBullet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19917f-3017-4b34-8756-f074313442c2",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "The goal of this lab is to populate the pybullet server with real-time information from the Webots world and check whether there is a collision with projected motions of the robot in order to determine suitable grasps. \n",
    "\n",
    "You are provided with an URDF files of the crate and  the Robotiq gripper. You can use the gripper coordinate system to locate the crate and then perform relative motions with the gripper. You are also provided with a perfect object detection pipeline that segments all objects in the crate. \n",
    "\n",
    "Steps:\n",
    "\n",
    "- Gain an understanding of how to load URDF and basic geometric objects into the PyBullet physics environment\n",
    "\n",
    "- Understand the two ways that you can compute collisions: computing the closest points between two objects (does not require simulation step) and computing the actual contact points.\n",
    "\n",
    "- Compute an Open3D point cloud, down sample it and approximate it with cubic or cyldrical objects. Also populate the box URDF into the same coordinate system. \n",
    "\n",
    "- Pick a single can, obtain its bounding object, and compute the position and orientation for grasping it. For this lab, you can limit yourself to grasping the cans at the center along its shorter width. \n",
    "\n",
    "- Test whether the grasp is feasible or not. If yes, execute it. If not, pick a new grasp. If no grasp is feasible, you can agitate the crate. \n",
    "\n",
    "Deliverable: A Python script that cycles through possible grasps and select a can it can grasps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619e98c-3c38-434a-8caf-31412c1c1028",
   "metadata": {},
   "source": [
    "# PyBullet\n",
    "\n",
    "PyBullet is the Python binding of the \"Bullet\" physics library. There exists multiple competing open-source physic simulation engines, another is the \"Open Dynamics Engine\" (ODE). This is the basis for the Webots simulator\n",
    "\n",
    "This tutorial is focussed on getting you the necessary tools to use PyBullet to help your robot to reason about the physical world, such as detecting collisions or how physical objects might behave when the robot interact with it. Please refer to the PyBullet quickstart https://usermanual.wiki/Document/pybullet20quickstart20guide.479068914/html#pf28 for more information, specifically if you are interested how to simulate multi-body physics (arms), cameras, range finders etc. and to get an idea how Webots simulates these systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9722d9e-ae02-47b6-9730-9d208a40abf5",
   "metadata": {},
   "source": [
    "## Example 1: Loading Objects from URDF\n",
    "\n",
    "You can export URDF from Webots by right-clicking on an object in the scene tree. This example uses the Robotiq gripper and the fruit bowl that we are using in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46315356-5a89-4fa3-a297-ddb27031feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import time\n",
    "import pybullet_data\n",
    "\n",
    "physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version\n",
    "\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally\n",
    "p.setGravity(0,0,-10) # set to (0,0,-10) for realistic gravity\n",
    "planeId = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "startPosCrate = [0,0,0.02]\n",
    "startOrientationCrate = p.getQuaternionFromEuler([0,0,0])\n",
    "crateId = p.loadURDF(\"crate.urdf\",startPosCrate, startOrientationCrate)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for x in np.linspace(-0.15,0.15,20):\n",
    "    startPos = [x,0,0.3]\n",
    "    startOrientation = p.getQuaternionFromEuler([-1.57,0,0])\n",
    "    gripperId = p.loadURDF(\"robotiq.urdf\",startPos, startOrientation)\n",
    "    p.resetBasePositionAndOrientation(crateId,startPosCrate,startOrientationCrate)    \n",
    "    closestPoints=p.getClosestPoints(gripperId,crateId,0.01) # does not requrire simulation step\n",
    "    if(closestPoints):\n",
    "        print(\"Objects are in collision\")\n",
    "    else:\n",
    "        print(\"Not in collision\")\n",
    "#    p.stepSimulation()    \n",
    "    time.sleep(0.001)\n",
    "    p.removeBody(gripperId)\n",
    "    \n",
    "    \n",
    "#print(p.getMatrixFromQuaternion(startOrientation))\n",
    "# It is also possible to use 'multiplyTransforms()' and 'invertTransforms()'\n",
    "\n",
    "gripperId = p.loadURDF(\"robotiq.urdf\",startPos, startOrientation)\n",
    "\n",
    "for i in range (10000):\n",
    "    p.stepSimulation()\n",
    "    time.sleep(1./240.)\n",
    "    p.resetBasePositionAndOrientation(crateId,startPosCrate,startOrientationCrate)\n",
    "    if i%100 == 0:\n",
    "        contacts=p.getContactPoints(gripperId,crateId)\n",
    "        if contacts:\n",
    "            print(\"Collision\")\n",
    "\n",
    "cubePos, cubeOrn = p.getBasePositionAndOrientation(bowlId)\n",
    "print(cubePos,cubeOrn)\n",
    "p.disconnect()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff68ace-3154-4bcf-96bc-fb32a4a7ffdf",
   "metadata": {},
   "source": [
    "## Example 2: Inserting standard geometries \n",
    "\n",
    "You can also place standard geometries in the environment. This can be useful to define simple collision objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb320d-e561-48b4-b025-abe70552a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import time\n",
    "import pybullet_data\n",
    "\n",
    "p.connect(p.GUI)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.createCollisionShape(p.GEOM_PLANE)\n",
    "p.createMultiBody(0, 0)\n",
    "\n",
    "sphereRadius = 0.05\n",
    "colSphereId = p.createCollisionShape(p.GEOM_SPHERE, radius=sphereRadius)\n",
    "colBoxId = p.createCollisionShape(p.GEOM_BOX,\n",
    "                                  halfExtents=[sphereRadius, sphereRadius, sphereRadius])\n",
    "\n",
    "mass = 1\n",
    "visualShapeId = -1\n",
    "\n",
    "link_Masses = [1]\n",
    "linkCollisionShapeIndices = [colBoxId]\n",
    "linkVisualShapeIndices = [-1]\n",
    "linkPositions = [[0, 0, 0.11]]\n",
    "linkOrientations = [[0, 0, 0, 1]]\n",
    "linkInertialFramePositions = [[0, 0, 0]]\n",
    "linkInertialFrameOrientations = [[0, 0, 0, 1]]\n",
    "indices = [0]\n",
    "jointTypes = [p.JOINT_REVOLUTE]\n",
    "axis = [[0, 0, 1]]\n",
    "\n",
    "for i in range(3):\n",
    "  for j in range(3):\n",
    "    for k in range(3):\n",
    "      basePosition = [\n",
    "          1 + i * 5 * sphereRadius, 1 + j * 5 * sphereRadius, 1 + k * 5 * sphereRadius + 1\n",
    "      ]\n",
    "      baseOrientation = [0, 0, 0, 1]\n",
    "      if (k & 2):\n",
    "        sphereUid = p.createMultiBody(mass, colSphereId, visualShapeId, basePosition,\n",
    "                                      baseOrientation)\n",
    "      else:\n",
    "        sphereUid = p.createMultiBody(mass,\n",
    "                                      colBoxId,\n",
    "                                      visualShapeId,\n",
    "                                      basePosition,\n",
    "                                      baseOrientation,\n",
    "                                      linkMasses=link_Masses,\n",
    "                                      linkCollisionShapeIndices=linkCollisionShapeIndices,\n",
    "                                      linkVisualShapeIndices=linkVisualShapeIndices,\n",
    "                                      linkPositions=linkPositions,\n",
    "                                      linkOrientations=linkOrientations,\n",
    "                                      linkInertialFramePositions=linkInertialFramePositions,\n",
    "                                      linkInertialFrameOrientations=linkInertialFrameOrientations,\n",
    "                                      linkParentIndices=indices,\n",
    "                                      linkJointTypes=jointTypes,\n",
    "                                      linkJointAxis=axis)\n",
    "\n",
    "      p.changeDynamics(sphereUid,\n",
    "                       -1,\n",
    "                       spinningFriction=0.001,\n",
    "                       rollingFriction=0.001,\n",
    "                       linearDamping=0.0)\n",
    "      for joint in range(p.getNumJoints(sphereUid)):\n",
    "        p.setJointMotorControl2(sphereUid, joint, p.VELOCITY_CONTROL, targetVelocity=1, force=10)\n",
    "\n",
    "p.setGravity(0, 0, -10)\n",
    "p.setRealTimeSimulation(1)\n",
    "\n",
    "p.getNumJoints(sphereUid)\n",
    "for i in range(p.getNumJoints(sphereUid)):\n",
    "  p.getJointInfo(sphereUid, i)\n",
    "\n",
    "while (1):\n",
    "  keys = p.getKeyboardEvents()\n",
    "  print(keys)\n",
    "\n",
    "  time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17974f-6fd9-41ea-a597-7af641d0e53b",
   "metadata": {},
   "source": [
    "# Webots Recognition Node\n",
    "\n",
    "Webots provides built-in object detection capability that works very similar to pixel-based segmentation tools such as YoLo, Mask R-CNN or Facebook's Detectron. Read up about it here https://www.cyberbotics.com/doc/reference/recognition\n",
    "\n",
    "In this lab, we provide you with the ability to recognize the cans and pick out the one that you want to grasp. Specifically, the cans each have a distinct \"recognition color\". This emulates perfect segmentation such as can be obtained by DBScan or similar algorithms. All recognition colors are composed of 0, 0.5 and 1 (e.g., [0 0.5 0] or [1 1 0.5]).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38dded-2756-4737-a5d6-bdacdd2a1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controller import Robot\n",
    "import numpy as np\n",
    "\n",
    "# create the Robot instance.\n",
    "robot = Robot()\n",
    "\n",
    "# get the time step of the current world.\n",
    "timestep = int(robot.getBasicTimeStep())\n",
    "\n",
    "speed=1.0\n",
    "\n",
    "start_pos = [0,-1.382,-1.13, -2,1.63,3.142]\n",
    "\n",
    "hand_motors=[robot.getDevice(\"finger_1_joint_1\"),\n",
    "             robot.getDevice(\"finger_2_joint_1\"),\n",
    "             robot.getDevice(\"finger_middle_joint_1\")]\n",
    "\n",
    "ur_motors=[robot.getDevice(\"shoulder_pan_joint\"),\n",
    "           robot.getDevice(\"shoulder_lift_joint\"),\n",
    "           robot.getDevice(\"elbow_joint\"),\n",
    "           robot.getDevice(\"wrist_1_joint\"),\n",
    "           robot.getDevice(\"wrist_2_joint\"),\n",
    "           robot.getDevice(\"wrist_3_joint\")]\n",
    "\n",
    "for i, ur_motor in enumerate(ur_motors):\n",
    "    ur_motor.setVelocity(speed)\n",
    "    ur_motor.setPosition(start_pos[i])\n",
    "\n",
    "position_sensors=[robot.getDevice(\"shoulder_pan_joint_sensor\"),\n",
    "                  robot.getDevice(\"shoulder_lift_joint_sensor\"),\n",
    "                  robot.getDevice(\"elbow_joint_sensor\"),\n",
    "                  robot.getDevice(\"wrist_1_joint_sensor\"),\n",
    "                  robot.getDevice(\"wrist_2_joint_sensor\"),\n",
    "                  robot.getDevice(\"wrist_3_joint_sensor\")]\n",
    "\n",
    "for position_sensor in position_sensors:\n",
    "    position_sensor.enable(timestep)\n",
    "    \n",
    "camera = robot.getDevice(\"camera\")\n",
    "camera.enable(timestep)\n",
    "camera.recognitionEnable(timestep)\n",
    "camera.enableRecognitionSegmentation()\n",
    "\n",
    "rangefinder = robot.getDevice(\"range-finder\")\n",
    "rangefinder.enable(timestep)                     \n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    robot.step(timestep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5678ab-2e3d-4709-96c5-97ef0270be78",
   "metadata": {},
   "source": [
    "## Read depth and segmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7329d93-163e-40bb-8d28-ef8fed097b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_1darray = np.frombuffer(rangefinder.getRangeImage(data_type=\"buffer\"), dtype=np.float32)\n",
    "depth=np.reshape(depth_1darray,(240,320))\n",
    "depth=depth*1000.0\n",
    "\n",
    "image_1darray = camera.getRecognitionSegmentationImage()\n",
    "image = np.frombuffer(image_1darray, np.uint8).reshape((camera.getHeight(), camera.getWidth(), 4))\n",
    "\n",
    "mono = np.dot(image,[0.2989, 0.5870, 0.1140,0]).astype(int)\n",
    "\n",
    "robot.step(timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f0d65-b694-4c23-8103-4d37a7d973d9",
   "metadata": {},
   "source": [
    "## Mask depth image with segmented image\n",
    "\n",
    "The goal of segmentation is to make individual objects uniquely identifiable. In this case, the segmentation routine will label each object's pixel with a unique color. We can use this information to mask these objects in the depth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbd6b6-9a8c-4338-8dec-8b78d95a71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Camera image')\n",
    "plt.imshow(image)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Depth image')\n",
    "plt.imshow(depth)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Monochrome image')\n",
    "plt.imshow(mono)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32273511-0e5d-4950-95cc-75af1bf6dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_colors=np.unique(mono).tolist()\n",
    "unique_colors.pop(0) # remove the first color - zero\n",
    "\n",
    "fig, ax = plt.subplots(len(unique_colors),1,figsize=(24,32))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, color in enumerate(unique_colors):\n",
    "    cand=np.multiply(depth,mono == color)\n",
    "    ax[i].imshow(cand)\n",
    "#    plt.title('Color {}'.format(color))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3923-c726-4b45-9b65-0d391111a848",
   "metadata": {},
   "source": [
    "# Compute Object Pose from Open3D Pointcloud\n",
    "\n",
    "We can pick individual objects by multiplying the depth image with a mask. We can hence compute the oriented bounding box and the relative pose of the object for the robot to grasp. \n",
    "\n",
    "The oriented bounding box data structure provides the center (.center) and orientation (.R), which allows you to compute its pose in the robot's base frame. You can then compute an IK solution that moves the robot arm there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4f809-6c17-405b-87e2-6ba4a9ef8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand=np.multiply(depth,mono == 254) # pick the white can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ace3a-36e3-4a3c-8a70-c3edba55eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        o3d.geometry.Image(image), \n",
    "        o3d.geometry.Image(np.array(cand).astype('uint16')),\n",
    "        convert_rgb_to_intensity=False,\n",
    "        depth_scale=1000.0, depth_trunc=1.5)\n",
    "\n",
    "\n",
    "can = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    rgbd_image,\n",
    "       o3d.camera.PinholeCameraIntrinsic(320,240,320,240,160,120),\n",
    "    project_valid_depth_only=True\n",
    ")\n",
    "\n",
    "can.paint_uniform_color([1.0, 0, 0]) # you need to color this as the segmentation color is unreliable, in particular for the white object\n",
    "\n",
    "#o3d.visualization.draw_geometries([can])\n",
    "\n",
    "obb = can.get_oriented_bounding_box()\n",
    "obb.color = (0, 1, 0)\n",
    "#o3d.visualization.draw_geometries([obb,can])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa7db7-1537-4e72-a260-e553f423febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the transformation matrix from rotation, translation, and [0 0 0 1]\n",
    "\n",
    "Randt=np.concatenate((obb.R, np.expand_dims(obb.center, axis=1)),axis=1) # pitfall: arrays need to be passed as a tuple\n",
    "lastrow=np.expand_dims(np.array([0,0,0,1]),axis=0)\n",
    "T=np.concatenate((Randt,lastrow))\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a1076-87fd-415b-8d0d-3d57661c13a4",
   "metadata": {},
   "source": [
    "# Compute grasp pose using Forward Kinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0f41e-77c7-4775-ab19-1a04ed2d6356",
   "metadata": {},
   "source": [
    "So far, we computed the position of the can only in end-effector space. The robot's inverse kinematics expects coordinates in the robot's frame. Note that the robot's frame that the IK solver is using is not visible in Webots. Its origin is around 7cm above the origin of the UR5e's base in Webots. \n",
    "\n",
    "Obviously, we cannot directly move the end-effector to the can. We will need to come a little from above, taking into account the lengt of the gripper. By experimenting with the system, we find out that we need to rotate the can by 90 degrees in order to grasp it along the narrow dimension. \n",
    "\n",
    "You will need to apply one more rotation to have the gripper point down, not up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcf466-e82d-4aa2-b4a9-5e96773a92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T[2,3]-=0.30 # move 20cm above the can\n",
    "\n",
    "q=-np.pi/2\n",
    "Rz=[[np.cos(q),-np.sin(q),0,0],\n",
    "    [np.sin(q),np.cos(q),0,0],\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1]]\n",
    "print(T)\n",
    "T=np.matmul(T,Rz)\n",
    "\n",
    "#\n",
    "#\n",
    "#  Add another rotation here\n",
    "#\n",
    "#\n",
    "\n",
    "print(T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251128b-4a4b-41b4-b7cc-5052dede20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmath\n",
    "import math\n",
    "from math import cos as cos\n",
    "from math import sin as sin\n",
    "from math import atan2 as atan2\n",
    "from math import acos as acos\n",
    "from math import asin as asin\n",
    "from math import sqrt as sqrt\n",
    "from math import pi as pi\n",
    "\n",
    "global mat\n",
    "mat=np.matrix\n",
    "\n",
    "global d, a, alph\n",
    "\n",
    "d = mat([0.1625, 0, 0, 0.1333, 0.0997, 0.0996]) #ur5e\n",
    "a = mat([0 ,-0.425 ,-0.3922 ,0 ,0 ,0]) #ur5e\n",
    "alph = mat([math.pi/2, 0, 0, math.pi/2, -math.pi/2, 0 ])  #ur5e\n",
    "\n",
    "# ************************************************** FORWARD KINEMATICS\n",
    "\n",
    "def AH( n,th,c  ):\n",
    "\n",
    "  T_a = mat(np.identity(4), copy=False)\n",
    "  T_a[0,3] = a[0,n-1]\n",
    "  T_d = mat(np.identity(4), copy=False)\n",
    "  T_d[2,3] = d[0,n-1]\n",
    "\n",
    "  Rzt = mat([[cos(th[n-1,c]), -sin(th[n-1,c]), 0 ,0],\n",
    "\t         [sin(th[n-1,c]),  cos(th[n-1,c]), 0, 0],\n",
    "\t         [0,               0,              1, 0],\n",
    "\t         [0,               0,              0, 1]],copy=False)\n",
    "      \n",
    "\n",
    "  Rxa = mat([[1, 0,                 0,                  0],\n",
    "\t\t\t [0, cos(alph[0,n-1]), -sin(alph[0,n-1]),   0],\n",
    "\t\t\t [0, sin(alph[0,n-1]),  cos(alph[0,n-1]),   0],\n",
    "\t\t\t [0, 0,                 0,                  1]],copy=False)\n",
    "\n",
    "  A_i = T_d * Rzt * T_a * Rxa\n",
    "\t    \n",
    "\n",
    "  return A_i\n",
    "\n",
    "def HTrans(th,c ):  \n",
    "  A_1=AH( 1,th,c  )\n",
    "  A_2=AH( 2,th,c  )\n",
    "  A_3=AH( 3,th,c  )\n",
    "  A_4=AH( 4,th,c  )\n",
    "  A_5=AH( 5,th,c  )\n",
    "  A_6=AH( 6,th,c  )\n",
    "      \n",
    "  T_06=A_1*A_2*A_3*A_4*A_5*A_6\n",
    "  return T_06\n",
    "\n",
    "def get_joint_angles():\n",
    "    th=[]\n",
    "    for position_sensor in position_sensors:\n",
    "        th.append(position_sensor.getValue())\n",
    "    return th\n",
    "\n",
    "def restorePose(angles):\n",
    "    for i,ur_motor in enumerate(ur_motors):\n",
    "        ur_motor.setPosition(angles[i])\n",
    "\n",
    "    for i in range(200):\n",
    "        robot.step(timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ac921-1d6f-435d-8ad6-4efc7b4be0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtain current joint angles\n",
    "# 2. compute end-effector pose\n",
    "# 3. transform desired pose into robot frame\n",
    "robot.step(timestep)\n",
    "angles=get_joint_angles()\n",
    "theta=np.matrix([[theta] for theta in angles])\n",
    "c=[0]\n",
    "InitialPose=HTrans(theta,c)\n",
    "\n",
    "DesiredPose=np.matmul(HTrans(theta,c),T) # Transform can pose T into robot coordinate frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567356d-8626-441e-9ecb-e6ee2b85d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(InitialPose,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c5125-4297-4365-aa0d-760f65ec819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(DesiredPose,decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330a6c2-6f56-43eb-bad7-41eb41802c30",
   "metadata": {},
   "source": [
    "# Implement motion\n",
    "\n",
    "We can now use IK to compute the joint angles that we need to get to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b83677-94fd-497d-90cb-844bd9052f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** INVERSE KINEMATICS \n",
    "\n",
    "from numpy import linalg\n",
    "\n",
    "global d1, a2, a3, a7, d4, d5, d6\n",
    "d1 =  0.1625\n",
    "a2 = -0.425\n",
    "a3 = -0.3922\n",
    "d4 =  0.1333\n",
    "d5 =  0.0997\n",
    "d6 =  0.0996\n",
    "\n",
    "def invKine(desired_pos):# T60\n",
    "  th = mat(np.zeros((6, 8)))\n",
    "  P_05 = (desired_pos * mat([0,0, -d6, 1]).T-mat([0,0,0,1 ]).T)\n",
    "  \n",
    "  # **** theta1 ****\n",
    "  \n",
    "  psi = atan2(P_05[2-1,0], P_05[1-1,0])\n",
    "  phi = acos(d4 /sqrt(P_05[2-1,0]*P_05[2-1,0] + P_05[1-1,0]*P_05[1-1,0]))\n",
    "  #The two solutions for theta1 correspond to the shoulder\n",
    "  #being either left or right\n",
    "  th[0, 0:4] = pi/2 + psi + phi\n",
    "  th[0, 4:8] = pi/2 + psi - phi\n",
    "  th = th.real\n",
    "  \n",
    "  # **** theta5 ****\n",
    "  \n",
    "  cl = [0, 4]# wrist up or down\n",
    "  for i in range(0,len(cl)):\n",
    "\t      c = cl[i]\n",
    "\t      T_10 = linalg.inv(AH(1,th,c))\n",
    "\t      T_16 = T_10 * desired_pos\n",
    "\t      th[4, c:c+2] = + acos((T_16[2,3]-d4)/d6);\n",
    "\t      th[4, c+2:c+4] = - acos((T_16[2,3]-d4)/d6);\n",
    "\n",
    "  th = th.real\n",
    "  \n",
    "  # **** theta6 ****\n",
    "  # theta6 is not well-defined when sin(theta5) = 0 or when T16(1,3), T16(2,3) = 0.\n",
    "\n",
    "  cl = [0, 2, 4, 6]\n",
    "  for i in range(0,len(cl)):\n",
    "\t      c = cl[i]\n",
    "\t      T_10 = linalg.inv(AH(1,th,c))\n",
    "\t      T_16 = linalg.inv( T_10 * desired_pos )\n",
    "\t      th[5, c:c+2] = atan2((-T_16[1,2]/sin(th[4, c])),(T_16[0,2]/sin(th[4, c])))\n",
    "\t\t  \n",
    "  th = th.real\n",
    "\n",
    "  # **** theta3 ****\n",
    "  cl = [0, 2, 4, 6]\n",
    "  for i in range(0,len(cl)):\n",
    "\t      c = cl[i]\n",
    "\t      T_10 = linalg.inv(AH(1,th,c))\n",
    "\t      T_65 = AH( 6,th,c)\n",
    "\t      T_54 = AH( 5,th,c)\n",
    "\t      T_14 = ( T_10 * desired_pos) * linalg.inv(T_54 * T_65)\n",
    "\t      P_13 = T_14 * mat([0, -d4, 0, 1]).T - mat([0,0,0,1]).T\n",
    "\t      t3 = cmath.acos((linalg.norm(P_13)**2 - a2**2 - a3**2 )/(2 * a2 * a3)) # norm ?\n",
    "\t      th[2, c] = t3.real\n",
    "\t      th[2, c+1] = -t3.real\n",
    "\n",
    "  # **** theta2 and theta 4 ****\n",
    "\n",
    "  cl = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "  for i in range(0,len(cl)):\n",
    "\t      c = cl[i]\n",
    "\t      T_10 = linalg.inv(AH( 1,th,c ))\n",
    "\t      T_65 = linalg.inv(AH( 6,th,c))\n",
    "\t      T_54 = linalg.inv(AH( 5,th,c))\n",
    "\t      T_14 = (T_10 * desired_pos) * T_65 * T_54\n",
    "\t      P_13 = T_14 * mat([0, -d4, 0, 1]).T - mat([0,0,0,1]).T\n",
    "\t      \n",
    "\t      # theta 2\n",
    "\t      th[1, c] = -atan2(P_13[1], -P_13[0]) + asin(a3* sin(th[2,c])/linalg.norm(P_13))\n",
    "\t      # theta 4\n",
    "\t      T_32 = linalg.inv(AH( 3,th,c))\n",
    "\t      T_21 = linalg.inv(AH( 2,th,c))\n",
    "\t      T_34 = T_32 * T_21 * T_14\n",
    "\t      th[3, c] = atan2(T_34[1,0], T_34[0,0])\n",
    "  th = th.real\n",
    "\n",
    "  return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166f3cb-cfb5-4610-9ca2-05b86ada0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the solution with the lowest joint motion, weighted strongest by the shoulder lift joint\n",
    "score=[]\n",
    "iksolution=invKine(DesiredPose)\n",
    "for i in range(max(iksolution.shape)):\n",
    "    goal=iksolution[:,i]\n",
    "    goal=[angle.item() for angle in goal]\n",
    "    score.append(sum(np.multiply([3,5,4,3,2,1],np.sqrt(np.square(np.array(angles)-np.array(goal))))))\n",
    "    print(\"Solution: {} Score {}\".format(i,score[i]))\n",
    "    #restorePose(goal)\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9c2dc-abab-48a1-93a3-dd882ddee248",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_goal = iksolution[:,score.index(min(score))]\n",
    "best_goal=[angle.item() for angle in best_goal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9db38a-c31b-410b-b62b-359f50731783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restorePose(best_goal) # a cheap way to achieve moveJ\n",
    "restorePose(angles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
